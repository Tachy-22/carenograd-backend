AI SDK 5 Settings
AI SDK by Vercel
Foundations

Overview
Providers and Models
Prompts
Tools
Streaming
Agents
Getting Started

Navigating the Library
Next.js App Router
Next.js Pages Router
Svelte
Vue.js (Nuxt)
Node.js
Expo
AI SDK Core

Overview
Generating Text
Generating Structured Data
Tool Calling
Prompt Engineering
Settings
Embeddings
Image Generation
Transcription
Speech
Language Model Middleware
Provider & Model Management
Error Handling
Testing
Telemetry
AI SDK UI

Overview
Chatbot
Chatbot Message Persistence
Chatbot Tool Usage
Generative User Interfaces
Completion
Object Generation
Streaming Custom Data
Error Handling
Transport
Reading UIMessage Streams
Message Metadata
Stream Protocols
AI SDK RSC

Advanced

Reference

AI SDK Core

AI SDK UI

AI SDK RSC

Stream Helpers

AI SDK Errors

Migration Guides

Troubleshooting

AI SDK Core
Settings
Settings

Large language models (LLMs) typically provide settings to augment their output.

All AI SDK functions support the following common settings in addition to the model, the prompt, and additional provider-specific settings:


const result = await generateText({
  model: 'openai/gpt-4.1',
  maxOutputTokens: 512,
  temperature: 0.3,
  maxRetries: 5,
  prompt: 'Invent a new holiday and describe its traditions.',
});
Some providers do not support all common settings. If you use a setting with a provider that does not support it, a warning will be generated. You can check the warnings property in the result object to see if any warnings were generated.

maxOutputTokens

Maximum number of tokens to generate.

temperature

Temperature setting.

The value is passed through to the provider. The range depends on the provider and model. For most providers, 0 means almost deterministic results, and higher values mean more randomness.

It is recommended to set either temperature or topP, but not both.

In AI SDK 5.0, temperature is no longer set to 0 by default.
topP

Nucleus sampling.

The value is passed through to the provider. The range depends on the provider and model. For most providers, nucleus sampling is a number between 0 and 1. E.g. 0.1 would mean that only tokens with the top 10% probability mass are considered.

It is recommended to set either temperature or topP, but not both.

topK

Only sample from the top K options for each subsequent token.

Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.

presencePenalty

The presence penalty affects the likelihood of the model to repeat information that is already in the prompt.

The value is passed through to the provider. The range depends on the provider and model. For most providers, 0 means no penalty.

frequencyPenalty

The frequency penalty affects the likelihood of the model to repeatedly use the same words or phrases.

The value is passed through to the provider. The range depends on the provider and model. For most providers, 0 means no penalty.

stopSequences

The stop sequences to use for stopping the text generation.

If set, the model will stop generating text when one of the stop sequences is generated. Providers may have limits on the number of stop sequences.

seed

It is the seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.

maxRetries

Maximum number of retries. Set to 0 to disable retries. Default: 2.

abortSignal

An optional abort signal that can be used to cancel the call.

The abort signal can e.g. be forwarded from a user interface to cancel the call, or to define a timeout.

Example: Timeout


const result = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Invent a new holiday and describe its traditions.',
  abortSignal: AbortSignal.timeout(5000), // 5 seconds
});
headers

Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.

You can use the request headers to provide additional information to the provider, depending on what the provider supports. For example, some observability providers support headers such as Prompt-Id.


import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = await generateText({
  model: openai('gpt-4o'),
  prompt: 'Invent a new holiday and describe its traditions.',
  headers: {
    'Prompt-Id': 'my-prompt-id',
  },
});
The headers setting is for request-specific headers. You can also set headers in the provider configuration. These headers will be sent with every request made by the provider.

Previous
Prompt Engineering
Next
Embeddings
On this page
Settings
maxOutputTokens
temperature
topP
topK
presencePenalty
frequencyPenalty
stopSequences
seed
maxRetries
abortSignal
Example: Timeout
headers
Elevate your AI applications with Vercel.
Trusted by OpenAI, Replicate, Suno, Pinecone, and more.
Vercel provides tools and infrastructure to deploy AI apps and features at scale.
Resources
Docs
Cookbook
Providers
Showcase
GitHub
Discussions
More
Playground
Contact Sales
About Vercel
Next.js + Vercel
Open Source Software
GitHub
X
Legal
Privacy Policy
Â© 2025 Vercel, Inc.



AI SDK Core: Settings